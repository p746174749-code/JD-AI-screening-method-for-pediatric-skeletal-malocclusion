The paper is currently under review, and the code will be released subsequently.

ğŸ› ï¸ Installation 
We recommend using Miniconda or Anaconda to manage your Python environment to avoid dependency conflicts.
This project requires Python 3.8+ and CUDA 11.6. We recommend using conda to manage dependencies.

1. Clone the Repository

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

3. Create Environment
```bash
conda create -n multiview python=3.8 -y
conda activate multiview
```

4. Install PyTorch (CUDA 11.6)
Since the project uses a specific CUDA version, please install PyTorch using the following command:
```bash
pip install torch==1.13.0+cu116 torchvision==0.14.0+cu116 torchaudio==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116
```

5. Install Other Dependencies
Install the remaining libraries (timm, opencv, pandas, etc.) from the requirements.txt:
```bash
pip install -r requirements.txt
```

ğŸ“‚ Data Preparation
To ensure the multi-view fusion mechanism works correctly, the dataset must follow a specific directory structure.
1. Dataset Directory Structure
The model expects a nested folder structure where each sample (case) contains its respective view images.
```text
data/
â”œâ”€â”€ train/                # Training set
â”‚   â”œâ”€â”€ class_0/          # Category name
â”‚   â”‚   â”œâ”€â”€ 001/          # Unique ID for each case
â”‚   â”‚   â”‚   â”œâ”€â”€ 4.jpg     # 90Â° profile view
â”‚   â”‚   â”‚   â”œâ”€â”€ 3.jpg     # 45Â° profile view
â”‚   â”‚   â”‚   â””â”€â”€ 2.jpg     # frontal smile view
â”‚   â”‚   â”‚   â””â”€â”€ 1.jpg     # frontal view
â”‚   â”‚   â””â”€â”€ 002/
â”‚   â””â”€â”€ class_1/
â”‚   â”œâ”€â”€ class_2/
â””â”€â”€ val/                  # Training set       
```

